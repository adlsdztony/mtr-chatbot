services:
    chatbot:
        build: .
        container_name: mtr-chatbot
        expose:
            - "8501"
        volumes:
            - .:/app
            - pip_cache:/root/.cache/pip
            - chroma_cache:/root/.cache/chroma
        environment:
            - OLLAMA_BASE_URL=http://host.docker.internal:11434
            - PYTHONPATH=/app
            - OLLAMA_CHAT_MODEL=qwen3:30b
        depends_on:
            - ollama
        networks:
            - mtr-network
        restart: unless-stopped
        command: >
            bash -c "
              streamlit run frontend/frontend.py --server.address 0.0.0.0 --server.port 8501
            "

    nginx:
        image: nginx:alpine
        container_name: mtr-nginx
        ports:
            - "8502:80"
        volumes:
            - ./nginx.conf:/etc/nginx/conf.d/default.conf:ro
            - ./.data/original:/usr/share/nginx/html:ro
        depends_on:
            - chatbot
        networks:
            - mtr-network
        restart: unless-stopped

volumes:
    pip_cache:
    chroma_cache:

networks:
    mtr-network:
        driver: bridge
