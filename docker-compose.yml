services:
    chatbot:
        build: .
        container_name: mtr-chatbot
        network_mode: host
        volumes:
            - .:/app
            - chroma_cache:/root/.cache/chroma
        environment:
            - OLLAMA_BASE_URL=http://localhost:11436
            - PYTHONPATH=/app
            - OLLAMA_CHAT_MODEL=qwen3:30b
        restart: unless-stopped
        command: >
            bash -c "
              uv run streamlit run frontend/frontend.py --server.address 0.0.0.0 --server.port 38502
            "

    nginx:
        image: nginx:alpine
        container_name: mtr-nginx
        network_mode: host
        volumes:
            - ./nginx.conf:/etc/nginx/conf.d/default.conf:ro
            - ./.data/original:/usr/share/nginx/html:ro
        depends_on:
            - chatbot
        restart: unless-stopped

volumes:
    chroma_cache: